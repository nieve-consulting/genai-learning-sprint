{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "6646706b",
      "metadata": {
        "id": "6646706b"
      },
      "source": [
        "# Prompt Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5dd0a36",
      "metadata": {
        "id": "f5dd0a36"
      },
      "source": [
        "Prompt Engineering is a nuanced practice that involves formulating questions and statements to effectively communicate with AI, specifically Large Language Models. It's a blend of analytical thinking and creativity, essential for translating human intentions into a language that AI can process and respond to accurately. This skill is crucial across a spectrum of AI applications, from automating creative tasks to enhancing technical analyses, ensuring that the AI's responses are as relevant and useful as possible."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f88e989",
      "metadata": {
        "id": "3f88e989"
      },
      "source": [
        "# Linguistics"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ff2f1d7",
      "metadata": {
        "id": "3ff2f1d7"
      },
      "source": [
        "Linguistics is the study of language. They play a key role in prompt engineering. Understanding the nuances of language and how it is used in different contexts is crucial for crafting effective prompts. Additionally, knowing hoe to use grammar and language structure that is universally used will result in the AI system returning back the most accurate results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZaaSGK7XhKJ0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZaaSGK7XhKJ0",
        "outputId": "054a87df-8843-49ad-8c1c-b8864214995e"
      },
      "outputs": [],
      "source": [
        "!pip install -U transformers accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "134429d9",
      "metadata": {
        "id": "134429d9"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline\n",
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0bb324f2",
      "metadata": {
        "id": "0bb324f2"
      },
      "source": [
        "# **What is a Quantized Model?**\n",
        "Quantization: This is a process that reduces the precision of the numbers used in a neural network. Typically, neural networks use 32-bit floating-point numbers for their weights and activations. Quantization involves converting these 32-bit numbers into a lower bit format, such as 8-bit integers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6803dfd1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "a11a3a4cd65f4de6bde809e1f84d947b",
            "1d12be049b114dc6983b1c2d6e0ac1d5",
            "8919f3ad245e4eebab9eec8bdea5ed24",
            "fa3fa0df11ca43e29a9b233dfb7590cb",
            "a4172438347b40988e85f83b0df2eb0a",
            "c8fdf53798e94d72aaaedaa5aa42436c",
            "4f4e75a86b8d49c382fda4693897e0b0",
            "f96a9ca207eb433282fe1bee1c6e9f2a",
            "e2a50a4408b343758741fb54bfd1ff31",
            "60b1ec43a83742049a2e82df4ac7f790",
            "c81636a2134a41cf95be55d72f792b79"
          ]
        },
        "id": "6803dfd1",
        "outputId": "f6270ff9-5c5e-4aa2-aa5c-b1c08383c09f"
      },
      "outputs": [],
      "source": [
        "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_8bit=True,\n",
        ")\n",
        "# quantization_config = BitsAndBytesConfig(\n",
        "#     load_in_4bit=True,\n",
        "#     bnb_4bit_quant_type=\"nf4\",\n",
        "#     bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "#     bnb_4bit_use_double_quant=True,\n",
        "# )\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=quantization_config,\n",
        "    low_cpu_mem_usage=True,\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "generator = pipeline('text-generation', model=model, tokenizer=tokenizer, truncation=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb9661c9",
      "metadata": {
        "id": "fb9661c9"
      },
      "source": [
        "## Types of Prompts"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad3e6f87",
      "metadata": {
        "id": "ad3e6f87"
      },
      "source": [
        "### Discrete Prompts"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "783e4598",
      "metadata": {
        "id": "783e4598"
      },
      "source": [
        "Discrete prompts are explicit instructions or questions directed to the language model. These can range from simple commands to complex questions. The effectiveness of a prompt can significantly impact the quality of the model's output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e09d210",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1e09d210",
        "outputId": "57759cc3-bde3-4984-cc70-97ba53f74bf8"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "This prompt is discrete because it provides a clear and direct\n",
        "task for the model – to compose a poem, and it specifies the\n",
        "subject of the poem – the ocean. There's no ambiguity in what\n",
        "is being asked, making it easier for the model to understand\n",
        "and respond effectively.\n",
        "\"\"\"\n",
        "discrete_prompt =  \"Write a beautiful poem about the ocean.\"\n",
        "response = generator(discrete_prompt, max_length=150)\n",
        "print(response[0]['generated_text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b83ac551",
      "metadata": {
        "id": "b83ac551"
      },
      "source": [
        "### Soft Prompts"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cfc2ac63",
      "metadata": {
        "id": "cfc2ac63"
      },
      "source": [
        "Soft prompts are more about suggestion than direct instruction. They nudge the model towards a certain tone, style, or topic without being explicit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2aaa5d42",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2aaa5d42",
        "outputId": "34ebf621-5339-47b1-eb2a-b387bde3c92d"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "In this example, the prompt doesn't directly ask the model to do\n",
        "anything specific, like write a poem or a story. Instead, it sets a\n",
        "scene and a mood — serenity and peace by the sea — and gently nudges\n",
        "the model to generate content in line with this imagery and atmosphere.\n",
        "This approach leaves more room for creative interpretation compared to\n",
        "a discrete prompt.\n",
        "\"\"\"\n",
        "soft_prompt =  \"Imagine a serene and peaceful day by the sea, where the waves gently lap against the shore.\"\n",
        "response = generator(soft_prompt, max_length=150)\n",
        "print(response[0]['generated_text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b94ab51",
      "metadata": {
        "id": "8b94ab51"
      },
      "source": [
        "## In-Context Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47fc46d4",
      "metadata": {
        "id": "47fc46d4"
      },
      "source": [
        "In-context learning refers to the model's ability to pick up on cues from the provided context to give an appropriate response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4c6b22d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4c6b22d",
        "outputId": "8129b7af-1d7f-46e5-eb9a-2126ae45a18d"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "In this example, the model uses the context provided in the\n",
        "prompt (John's actions and realization) to infer and predict what John\n",
        "is likely to do next. It understands from the context that John has\n",
        "just returned from an orchard, and upon realizing he wants oranges,\n",
        "it's logical to infer he might return to the orchard or visit a store\n",
        "to satisfy this new desire. This demonstrates the model's ability to\n",
        "use the given context to form a coherent and contextually appropriate\n",
        "response.\n",
        "\"\"\"\n",
        "in_context_prompt =  \"John went to the orchard to pick apples. He filled his basket and returned home. Once home, he realized he also wanted oranges. Question: What is John likely to do next?\"\n",
        "response = generator(in_context_prompt, max_length=100)\n",
        "print(response[0]['generated_text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e241b18",
      "metadata": {
        "id": "4e241b18"
      },
      "source": [
        "## Few-Shot Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5404bdf1",
      "metadata": {
        "id": "5404bdf1"
      },
      "source": [
        "Few-shot learning is about providing examples in the prompt to teach the model what kind of output you expect."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68b5995a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68b5995a",
        "outputId": "805a6824-6e94-427a-af68-75662233fc69"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "In this example, the first two sentences and their corresponding\n",
        "questions serve as examples for the model. The model is then expected\n",
        "to apply the learned pattern to a new sentence. By providing a few\n",
        "examples, the prompt guides the model in understanding the task of\n",
        "converting statements into questions, demonstrating few-shot learning.\n",
        "\"\"\"\n",
        "few_shot_prompt =  \"I will provide you with a few examples of converting sentences to questions. Follow the pattern to convert the given sentence into a question. Example 1: Sentence: 'He is playing guitar.' Question: 'Is he playing guitar?'Example 2: Sentence: 'They were watching a movie.' Question: 'Were they watching a movie?' Now, convert this sentence to a question: 'She was reading a book.' Response:\"\n",
        "response = generator(few_shot_prompt, max_length=200)\n",
        "print(response[0]['generated_text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0d1e9ea",
      "metadata": {
        "id": "b0d1e9ea"
      },
      "source": [
        "## Designing Effective Prompts"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30d36de4",
      "metadata": {
        "id": "30d36de4"
      },
      "source": [
        "Designing effective prompts is a skill that combines creativity with an understanding of the model's inner workings. Effective prompts need to be clear, relevant, specific, and well-balanced.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d63fe0c",
      "metadata": {
        "id": "6d63fe0c"
      },
      "source": [
        "## Good vs Bad Prompts\n",
        "Prompt engineering is not just about crafting a request to a language model; it's about doing so in a way that the model can understand and respond to effectively. The quality of the prompt can greatly influence the quality of the output. Here, we'll look at some characteristics of good and bad prompts and see examples of each.\n",
        "Characteristics of Good Prompts: Clear and Specific: Good prompts provide clear instructions or questions that are directly related to the desired output. Contextually Rich: They provide enough context to guide the model in the direction of the expected response. Balanced Detail: They include just the right amount of detail—enough to direct the model without over-constraining its responses. Aligned With Model's Training: They are aligned with the type of content and format the model was trained on. Characteristics of Bad Prompts: Vague or Ambiguous: Bad prompts are unclear, which can lead to irrelevant or off-topic responses from the model. Lacking Context: Without context, the model may not understand the prompt's intent or may provide generic answers. Overly Detailed or Restrictive: Prompts that are too specific may limit the model's ability to generate creative or comprehensive responses. Mismatched to Model's Training: Using a style or requesting information that the model wasn't trained on can result in poor performance."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66cfceb4",
      "metadata": {
        "id": "66cfceb4"
      },
      "source": [
        "### 1. Clarity"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a77e54a",
      "metadata": {
        "id": "7a77e54a"
      },
      "source": [
        "Clarity in a prompt means being direct and straightforward, avoiding ambiguity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ff29bc8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ff29bc8",
        "outputId": "5fc580f1-fe1d-4431-de27-798ae4d909d0"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "This prompt is specific and direct. It leads to focused and relevant responses from the AI.\n",
        "\"\"\"\n",
        "\n",
        "clear_prompt = \"Summarize the main events in the novel 'Tell Me Your Dreams' by Sydney Sheldon.\"\n",
        "\n",
        "response = generator(clear_prompt, truncation=True, max_length=500)\n",
        "print(response[0]['generated_text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bbe13b43",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbe13b43",
        "outputId": "fdc5baf3-b1b8-4d16-b632-6bc5b8909a7e"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "This prompt is broad and unspecific. It often results in generic or irrelevant answers because the AI\n",
        "lacks clear guidance.\n",
        "\"\"\"\n",
        "\n",
        "vague_prompt = \"Tell me something about a book.\"\n",
        "response = generator(vague_prompt, max_length=500)\n",
        "print(response[0]['generated_text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94b19e53",
      "metadata": {
        "id": "94b19e53"
      },
      "source": [
        "### 2. Relevance"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0327069c",
      "metadata": {
        "id": "0327069c"
      },
      "source": [
        "The prompt should be relevant to the task or the information you are seeking."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2457ad95",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2457ad95",
        "outputId": "5b004f15-5ffb-41d5-b753-de836de6c598"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "This prompt is direct and to the point. It asks for a specific conversion task, which is well within\n",
        "the AI's capabilities, ensuring an accurate and relevant response.\n",
        "\"\"\"\n",
        "\n",
        "relevant_prompt = \"Convert this temperature from Celsius to Fahrenheit: 25°C.\"\n",
        "response = generator(relevant_prompt, max_length=200)\n",
        "print(response[0]['generated_text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f4ef8f6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8f4ef8f6",
        "outputId": "75a3939b-5c41-4e0b-8f3f-8998eb7faecb"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "This prompt is vague and lacks specificity. Without details on location, time, or particular weather aspects,\n",
        "the AI's response could be too general or not what the user is actually looking for.\n",
        "\"\"\"\n",
        "\n",
        "irrelevant_prompt = \"Tell me about the weather.\"\n",
        "response = generator(irrelevant_prompt, max_length=200)\n",
        "print(response[0]['generated_text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "baa1c8c8",
      "metadata": {
        "id": "baa1c8c8"
      },
      "source": [
        "### 3. Specificity"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb94c54f",
      "metadata": {
        "id": "cb94c54f"
      },
      "source": [
        "Specific prompts guide the model more precisely, yielding more targeted responses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ce33593",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ce33593",
        "outputId": "40e7afd5-0f37-4efd-a7d5-4801c8b025bf"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "This prompt is very specific, providing clear guidance on the poem's theme (sunset, beach) and\n",
        "length (four lines). It helps the AI to generate a focused and relevant poetic piece.\n",
        "\"\"\"\n",
        "\n",
        "specific_prompt = \"Write a four-line poem about a sunset on the beach.\"\n",
        "response = generator(specific_prompt, max_length=200)\n",
        "print(response[0]['generated_text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bebd95c6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bebd95c6",
        "outputId": "ced317b7-35d6-4478-d93a-ca1c9b7e254c"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "This prompt is too general and open-ended. It lacks direction regarding theme, style, or length,\n",
        "leading to a wide range of possible outcomes, which may or may not align with the user's expectations.\n",
        "\"\"\"\n",
        "\n",
        "general_prompt = \"Write a poem.\"\n",
        "response = generator(general_prompt, max_length=200)\n",
        "print(response[0]['generated_text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c27b7817",
      "metadata": {
        "id": "c27b7817"
      },
      "source": [
        "### 4. Balance Between Too Vague and Too Detailed:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26ba1f9f",
      "metadata": {
        "id": "26ba1f9f"
      },
      "source": [
        "Finding a balance is key. Overly detailed prompts might restrict the model's creative potential, while overly vague ones might lead to irrelevant responses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f18e76b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2f18e76b",
        "outputId": "a9334984-e285-474c-ebd2-f5a8fb6edc68"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "This prompt is excessively detailed, focusing on very specific quantities and food items.\n",
        "It might limit the AI's response to only those parameters, potentially missing out on broader nutritional\n",
        "insights.\n",
        "\"\"\"\n",
        "\n",
        "overly_detailed_prompt = \"Describe the health benefits of eating 70 grams of broccoli, 50 grams of carrots, and 80 grams of chicken breast every day.\"\n",
        "response = generator(overly_detailed_prompt, max_length=500)\n",
        "print(response[0]['generated_text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8094dbfb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8094dbfb",
        "outputId": "4eb5d9fc-55be-4633-d884-304493ed1764"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "This prompt strikes a good balance. It's specific enough to focus on the health benefits of a balanced diet,\n",
        "yet broad enough to allow for a comprehensive discussion of various aspects of nutrition and health.\n",
        "\"\"\"\n",
        "\n",
        "balanced_prompt = \"Describe the health benefits of a balanced diet.\"\n",
        "response = generator(balanced_prompt, max_length=500)\n",
        "print(response[0]['generated_text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13440999",
      "metadata": {
        "id": "13440999"
      },
      "source": [
        "### 5. Influence of Word Choice and Structure:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e4284e6",
      "metadata": {
        "id": "1e4284e6"
      },
      "source": [
        "Different wordings and structures can lead the model down different paths."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e84ce860",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e84ce860",
        "outputId": "3f983873-2552-4a13-87a6-96eb27576019"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "This prompt is formally worded and structured, leading to a more academic or professional tone in the\n",
        "response. It guides the AI to provide a detailed and structured analysis of the economic consequences of\n",
        "global warming.\n",
        "\"\"\"\n",
        "\n",
        "prompt_formal = \"What are the economic impacts of global warming?\"\n",
        "response = generator(prompt_formal, max_length=500)\n",
        "print(response[0]['generated_text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "900cab7a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "900cab7a",
        "outputId": "12f81691-3650-4df3-945d-18857eeb2186"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "This prompt's informal phrasing sets a conversational tone. It may lead the AI to adopt a more casual,\n",
        "perhaps less technical approach while still addressing the impact of global warming on the economy.\n",
        "\"\"\"\n",
        "\n",
        "prompt_informal = \"How's global warming messing up the economy?\"\n",
        "response = generator(prompt_informal, max_length=500)\n",
        "print(response[0]['generated_text'])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1d12be049b114dc6983b1c2d6e0ac1d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c8fdf53798e94d72aaaedaa5aa42436c",
            "placeholder": "​",
            "style": "IPY_MODEL_4f4e75a86b8d49c382fda4693897e0b0",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "4f4e75a86b8d49c382fda4693897e0b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "60b1ec43a83742049a2e82df4ac7f790": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8919f3ad245e4eebab9eec8bdea5ed24": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f96a9ca207eb433282fe1bee1c6e9f2a",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e2a50a4408b343758741fb54bfd1ff31",
            "value": 2
          }
        },
        "a11a3a4cd65f4de6bde809e1f84d947b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1d12be049b114dc6983b1c2d6e0ac1d5",
              "IPY_MODEL_8919f3ad245e4eebab9eec8bdea5ed24",
              "IPY_MODEL_fa3fa0df11ca43e29a9b233dfb7590cb"
            ],
            "layout": "IPY_MODEL_a4172438347b40988e85f83b0df2eb0a"
          }
        },
        "a4172438347b40988e85f83b0df2eb0a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c81636a2134a41cf95be55d72f792b79": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c8fdf53798e94d72aaaedaa5aa42436c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e2a50a4408b343758741fb54bfd1ff31": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f96a9ca207eb433282fe1bee1c6e9f2a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fa3fa0df11ca43e29a9b233dfb7590cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_60b1ec43a83742049a2e82df4ac7f790",
            "placeholder": "​",
            "style": "IPY_MODEL_c81636a2134a41cf95be55d72f792b79",
            "value": " 2/2 [00:04&lt;00:00,  2.22s/it]"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
