{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGpjFGh2qX2O"
      },
      "source": [
        "# Resource Augmented Generation (RAG)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACA8jfiCDuOU"
      },
      "source": [
        "## Overview\n",
        "\n",
        "- Motivation for RAG\n",
        "- Idea behind RAG\n",
        "- Advantages and Disadvantages\n",
        "- Implementation to augment question + answer\n",
        "- Advanced applications\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5YjAA14ih8R"
      },
      "source": [
        "#### Imagine you went to live under a rock on August 2006. When you come out in 2024, you are asked how many planets revolve around the sun. What would you say?...\n",
        "\n",
        "![pluto](https://github.com/nieve-consulting/genai-learning-sprint/blob/main/labs/03-rag/images/pluto_planets.jpeg?raw=1)\n",
        "\n",
        "This is similar to LLMs which are trained with data until a certain point and then asked questions on data they are not trained on. Understandably, LLMs will either be unable to answer or simply hallucinate a probably wrong answer.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3HmhY-rl29V"
      },
      "source": [
        "### What can be done?\n",
        "\n",
        "Have the LLM go to the library using **Research Augmented Generation (RAG)**!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXtqMcK1nAv_"
      },
      "source": [
        "RAG involves adding your own data (via a retrieval tool) to the prompt that you pass into a large language model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAGt5yQVrz0C"
      },
      "source": [
        "![rag architecture](https://github.com/nieve-consulting/genai-learning-sprint/blob/main/labs/03-rag/images/rag-overview.original.png?raw=1)\n",
        "\n",
        "Image credit: https://scriv.ai/guides/retrieval-augmented-generation-overview/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6N4A87lt3Wq"
      },
      "source": [
        "RAG has been shown to improve LLM prediction accuracy without needing to increase parameter size.\n",
        "\n",
        "![rag architecture](https://github.com/nieve-consulting/genai-learning-sprint/blob/main/labs/03-rag/images/rag_acc_v_size.png?raw=1)\n",
        "\n",
        "_Image credit: Yu, Wenhao. \"Retrieval-augmented generation across heterogeneous knowledge.\" Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Student Research Workshop. 2022._\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDcwB4BMyUQ5"
      },
      "source": [
        "RAG also increases explainability by giving the source for information.\n",
        "\n",
        "![rag architecture](https://github.com/nieve-consulting/genai-learning-sprint/blob/main/labs/03-rag/images/rag_source_locator.png?raw=1)\n",
        "\n",
        "Image credit: https://ai.stanford.edu/blog/retrieval-based-NLP/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRUJ2eQvJIAg"
      },
      "source": [
        "## Advantages and Disadvantages\n",
        "\n",
        "### Advantages\n",
        "\n",
        "- Provides domain specific context\n",
        "- Improves predictive performance and reduces hallucinations\n",
        "- Does not increase model parameters\n",
        "- Less labor intensive than fine-tuning LLMs\n",
        "\n",
        "### Disadvantages\n",
        "\n",
        "- May introduce latency since we are adding a relatively costly search step\n",
        "- If your dataset includes private information, you may inadvertently expose another user with this information.\n",
        "- The data you want to use needs to be curated and you should decide how the data should be accessed. This adds time for the initial set-up.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s42Iqfmjy-1E"
      },
      "source": [
        "# Implementation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVkTgnU80jjV"
      },
      "source": [
        "### 1. Install + load relevant modules:\n",
        "\n",
        "- langchain\n",
        "- torch\n",
        "- transformers\n",
        "- sentence-transformers\n",
        "- datasets\n",
        "- faiss-cpu\n",
        "- pypdf\n",
        "- unstructure[pdf]\n",
        "- huggingface_hub (add hf_token)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DbjNohhBQrVz",
        "outputId": "a3fe2fa8-3997-4399-bdf1-0ac21f5686ff"
      },
      "outputs": [],
      "source": [
        "!apt install poppler-utils tesseract-ocr -y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1uDuyDLd1Xi",
        "outputId": "4da01879-fda7-430f-b5fb-38a1cd5f8e48"
      },
      "outputs": [],
      "source": [
        "!pip install langchain\n",
        "!pip install langchain-community\n",
        "!pip install torch\n",
        "!pip install transformers\n",
        "!pip install faiss-cpu\n",
        "!pip install pypdf\n",
        "!pip install sentence-transformers\n",
        "!pip install unstructured\n",
        "!pip install unstructured[pdf]\n",
        "!pip install tiktoken\n",
        "!pip install huggingface_hub\n",
        "!pip install --upgrade nltk\n",
        "!pip install --upgrade bitsandbytes\n",
        "!pip install einops\n",
        "!pip install xformers\n",
        "!pip install chromadb\n",
        "!pip install accelerate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5agp6iRm-7jD"
      },
      "source": [
        "### 2. Choose a dataset to use and then load it into your code\n",
        "\n",
        "Here we are using the pdfs loaded in pdfs/. We load this using langchain DirectoryLoader.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBx0cQJ9Jmtv"
      },
      "source": [
        "We can load multiple types of datasets into this example though the most commonly used are PDFs and websites.\n",
        "\n",
        "To load websites, we could also use `langchain WebBaseLoader`\n",
        "\n",
        "In this example, we will consider PDFs and load them in using `langchain DirectoryLoader`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VdEFWNptRp47",
        "outputId": "5cc428cb-986a-47a6-ca6b-4af049307bc8"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download(\"punkt_tab\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YYoEp4FJz6oK",
        "outputId": "2b751784-2672-4fe3-83df-59577dda5743"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import DirectoryLoader\n",
        "\n",
        "loader = DirectoryLoader(\"./PDFs\", glob=\"**/*.pdf\", show_progress=True)\n",
        "documents = loader.load()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjvyEOMMBB7G"
      },
      "source": [
        "### 3. Now, we need to split our documents into chunks.\n",
        "\n",
        "We want the embedding to be greater than 1 word but much less than an entire page. This is essential for the similarity search between the query and the document. Essentially, the query will be searched for greatest similarity to embedded chunks in the dataset. Then those chunks with greatest similarity are augmented to the query.\n",
        "\n",
        "It is essential to choose the chunking method according to your data type.\n",
        "There are different ways to do this:\n",
        "\n",
        "Fixed size\n",
        "\n",
        "- Token: Splits text on tokens. Can chunk tokens together\n",
        "- Character: Splits based on some user defined character.\n",
        "\n",
        "Recursive\n",
        "\n",
        "- Recursively splits text. Useful for keeping related pieces of text next to each other.\n",
        "\n",
        "Document based\n",
        "\n",
        "- HTML: Splits text based on HTML-specific characters.\n",
        "- Markdown: Splits on Markdown-specific characters\n",
        "- Code: Splits text based on characters specific to coding languages.\n",
        "\n",
        "Semantic chunking\n",
        "\n",
        "- Extract semantic meaning from embeddings and then assess the semantic relationship between these chunks. Essentially splits into sentences, then groups into groups of 3 sentences, and then merges one that are similar in the embedding space.\n",
        "\n",
        "Here we use recursive where the dataset is split using a set of characters. The default characters provided to it are [\"\\n\\n\", \"\\n\", \" \", \"\"]. A large text is split by the first character \\n\\n. If the first split by \\n\\n is still large then it moves to the next character which is \\n and tries to split by it. This continues until the chunk size is reached.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "SfgRcG6g0XOG"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)\n",
        "docs = text_splitter.split_documents(documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YyTRRvU6DboO"
      },
      "source": [
        "### 4. Then we embed the chunked texts using a Transformer.\n",
        "\n",
        "This allows us to encode the text into our search\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXny33dcCHZc"
      },
      "source": [
        "Embedding converts text to a numerical representation in a vector space. RAG compares the embeddings of user queries within the vector of the knowledge library.\n",
        "\n",
        "In this example, we choose a simple embedding using the MiniLM\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kabjUo1I1Pev",
        "outputId": "220b7cc5-7002-4ff3-d505-90d6f382edad"
      },
      "outputs": [],
      "source": [
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "modelPath = \"sentence-transformers/all-MiniLM-l6-v2\"\n",
        "model_kwargs = {\"device\": \"cpu\"}\n",
        "encode_kwargs = {\"normalize_embeddings\": False}\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=modelPath, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvkGZSJvEFeK"
      },
      "source": [
        "### 5.Create a vector database\n",
        "\n",
        "Vector databases, also called vector storage, efficiently store and retrieve vector data, which are arrays of numerical values representing points in multi-dimensional space. They're useful for handling data like embeddings from deep learning models or numerical features. Unlike traditional relational databases, which aren't optimized for vectors, vector databases offer efficient storage, indexing, and querying for high-dimensional and variable-length vectors.\n",
        "\n",
        "There are various types of vector databases:\n",
        "\n",
        "1. Chroma\n",
        "2. FAISS\n",
        "3. Pinecone\n",
        "4. Weaviate\n",
        "5. Qdrant\n",
        "\n",
        "Here, we build this using the FAISS utility.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NqccXGwoEYP-"
      },
      "source": [
        "![vector_database](https://github.com/nieve-consulting/genai-learning-sprint/blob/main/labs/03-rag/images/vector_database.png?raw=1)\n",
        "\n",
        "Image credit: https://blog.gopenai.com/primer-on-vector-databases-and-retrieval-augmented-generation-rag-using-langchain-pinecone-37a27fb10546\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aWu5YGPj1e6B",
        "outputId": "ca4c1557-07ab-479c-a9ea-716fb11c4f5e"
      },
      "outputs": [],
      "source": [
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "db = FAISS.from_documents(docs, embeddings)\n",
        "question = \"What is RF Fold?\"\n",
        "searchDocs = db.similarity_search(question)\n",
        "print(searchDocs[0].page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5A56JddAEkCF"
      },
      "source": [
        "### 6. Initialize the LLM that will be used for question answering\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdUokERIMnz3"
      },
      "source": [
        "Here, we use a pretrained model flan-t5-large as part of a HuggingFacePipeline. This will later be chained with the vector database for RAG.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9CXf3N6c134r",
        "outputId": "a104999f-94ad-434f-87e3-88a0fd684916"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "from langchain import HuggingFacePipeline\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-large\")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-large\")\n",
        "pipe = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)\n",
        "llm = HuggingFacePipeline(\n",
        "    pipeline=pipe,\n",
        "    model_kwargs={\"temperature\": 0, \"max_length\": 2048},\n",
        ")\n",
        "#'HuggingFaceH4/zephyr-7b-beta'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvJhhDzYEvhu"
      },
      "source": [
        "### 7. Retrieve data and use it to answer a question\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOQG22pHOf5W"
      },
      "source": [
        "![rag_workflow](https://github.com/nieve-consulting/genai-learning-sprint/blob/main/labs/03-rag/images/rag_workflow.png?raw=1)\n",
        "\n",
        "Image credit: https://blog.gopenai.com/retrieval-augmented-generation-101-de05e5dc21ef\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWJHxBc9hzJB"
      },
      "source": [
        "Let's ask questions it would only be able to know if the model actually read the texts!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "nMrN1_3B2cA1"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Keep the answer as concise as possible.\n",
        "{context}\n",
        "Question: {question}\n",
        "Helpful Answer:\"\"\"\n",
        "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HLCz3eiX2e0g",
        "outputId": "47160f36-d808-4f08-b9b4-d0d8d40ecb52"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=db.as_retriever(),\n",
        "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT},\n",
        ")\n",
        "result = qa_chain(\n",
        "    {\"query\": \"What technique proposed in 2023 can be used to predict protein folding?\"}\n",
        ")\n",
        "print(result[\"result\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynkOkS39K2FT"
      },
      "source": [
        "Now let's ask the chain where to find the article related to RFDiffusion\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GWP0ZJ0Cc7uw",
        "outputId": "223bdc72-1ad3-4801-f781-38a54d4a43e3"
      },
      "outputs": [],
      "source": [
        "qa_chain(\n",
        "    {\n",
        "        \"query\": \"Which scientific article should I read to learn about RFdiffusion for protein folding?\"\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_f-vDxEaMT2_"
      },
      "source": [
        "## Exercise\n",
        "\n",
        "Use any of the frameworks/models here to load in your favorite websites and ask the model a question regarding them.\n",
        "\n",
        "Hint:\n",
        "\n",
        "```\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "\n",
        "loader = WebBaseLoader([\"https://www.espn.com/\", \"https://www.google.com\"])\n",
        "\n",
        "To bypass SSL verification errors during fetching, you can set the “verify” option:\n",
        "\n",
        "loader.requests_kwargs = {‘verify’:False}\n",
        "\n",
        "data = loader.load()\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iviJn8067KQX"
      },
      "source": [
        "## More applications\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTNUrAkH9rsE"
      },
      "source": [
        "### RAG using Llama 2, Langchain and ChromaDB\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SzBLZA6-H1i"
      },
      "source": [
        "Load in models and setup pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "GgK306ND-pLc"
      },
      "outputs": [],
      "source": [
        "from torch import cuda, bfloat16\n",
        "import torch\n",
        "import transformers\n",
        "from transformers import AutoTokenizer\n",
        "from time import time\n",
        "\n",
        "# import chromadb\n",
        "# from chromadb.config import Settings\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "\n",
        "device = f\"cuda\"\n",
        "\n",
        "# set quantization configuration to load large model with less GPU memory\n",
        "# this requires the `bitsandbytes` library\n",
        "bnb_config = transformers.BitsAndBytesConfig(\n",
        "    load_in_4bit=False,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=bfloat16,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "f212bf7772464da3a3f2e9078ed4e2e7",
            "220dd6c6142f49309b580befad8037c8",
            "8445cffea19548d5bc7d8892f20915da",
            "e9286683b7e34ad5a330f02bccf406f4",
            "a7dc5828777e4a40adb816e9a3d0c11c",
            "770c05476b3843a0ac73e8bca990453a",
            "b06c2e4efe934233ba086b7d2863334e",
            "ac3f3d53dd7f45c18a8b792c2d8e4239",
            "dec521676dff47b1b594d1fd891ec81c",
            "d59d898fff02424baef83b8fcfa6b70f",
            "0b90e32e0f4449c78f63832f77aa0184"
          ]
        },
        "id": "GO0N4dvX-Gkm",
        "outputId": "5d6bdffd-1f69-4b44-d3a3-aea0d78378a7"
      },
      "outputs": [],
      "source": [
        "time_1 = time()\n",
        "model_config = transformers.AutoConfig.from_pretrained(\n",
        "    model_id,\n",
        ")\n",
        "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    trust_remote_code=True,\n",
        "    config=model_config,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "time_2 = time()\n",
        "print(f\"Prepare model, tokenizer: {round(time_2-time_1, 3)} sec.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hX_J_eSa_F9-",
        "outputId": "8ebc30dd-701a-41fa-ad74-e5ea0bf1bda4"
      },
      "outputs": [],
      "source": [
        "time_1 = time()\n",
        "query_pipeline = transformers.pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "time_2 = time()\n",
        "print(f\"Prepare pipeline: {round(time_2-time_1, 3)} sec.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXra3Pac_YjE"
      },
      "source": [
        "Prepare function to test pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "eK9y4p7-_X9t"
      },
      "outputs": [],
      "source": [
        "def test_model(tokenizer, pipeline, prompt_to_test):\n",
        "    \"\"\"\n",
        "    Perform a query\n",
        "    print the result\n",
        "    Args:\n",
        "        tokenizer: the tokenizer\n",
        "        pipeline: the pipeline\n",
        "        prompt_to_test: the prompt\n",
        "    Returns\n",
        "        None\n",
        "    \"\"\"\n",
        "    # adapted from https://huggingface.co/blog/llama2#using-transformers\n",
        "    time_1 = time()\n",
        "    sequences = pipeline(\n",
        "        prompt_to_test,\n",
        "        do_sample=True,\n",
        "        top_k=10,\n",
        "        num_return_sequences=1,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        max_length=200,\n",
        "    )\n",
        "    time_2 = time()\n",
        "    print(f\"Test inference: {round(time_2-time_1, 3)} sec.\")\n",
        "    for seq in sequences:\n",
        "        print(f\"Result: {seq['generated_text']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ceE5C8mU_hB3",
        "outputId": "d721482b-6705-49d6-c582-3cc71d3fe9f7"
      },
      "outputs": [],
      "source": [
        "test_model(\n",
        "    tokenizer,\n",
        "    query_pipeline,\n",
        "    \"Please explain what is the State of the Union address. Give just a definition. Keep it in 100 words.\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OK4yMFFN_dwA"
      },
      "source": [
        "Set up Huggingface pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "cqXPe5mZ_quI",
        "outputId": "c4dbd2e8-40e5-4f2e-cfa5-10de0d4eff70"
      },
      "outputs": [],
      "source": [
        "llm = HuggingFacePipeline(pipeline=query_pipeline)\n",
        "# checking again that everything is working fine\n",
        "llm(\n",
        "    prompt=\"Please explain what is the State of the Union address. Give just a definition. Keep it in 100 words.\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1vvVRwV_06d"
      },
      "source": [
        "Load in data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "bVn0hr1K_2cw"
      },
      "outputs": [],
      "source": [
        "loader = TextLoader(\"./state_union2023.txt\", encoding=\"utf8\")\n",
        "documents = loader.load()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXcI1FMTAVwn"
      },
      "source": [
        "Chunk data recursively\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "I0yCLaqbAXrr"
      },
      "outputs": [],
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
        "all_splits = text_splitter.split_documents(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KPolpwX5bQGs",
        "outputId": "fcdd216a-766f-4f64-a9a9-0e1aee3ebf24"
      },
      "outputs": [],
      "source": [
        "all_splits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vMMRTYEAZEx"
      },
      "source": [
        "Embed and store in Chroma Vector store\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wPX2j0W0AnRS",
        "outputId": "6c05ed9b-19fe-48df-8566-673b6de604ee"
      },
      "outputs": [],
      "source": [
        "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
        "model_kwargs = {\"device\": \"cpu\"}\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "3gZKaDW5BDDJ"
      },
      "outputs": [],
      "source": [
        "vectordb = Chroma.from_documents(\n",
        "    documents=all_splits, embedding=embeddings, persist_directory=\"chroma_db\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGKlWcClBJPX"
      },
      "source": [
        "Set up chain\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "8tHJsc-nAuyU"
      },
      "outputs": [],
      "source": [
        "retriever = vectordb.as_retriever()\n",
        "\n",
        "qa = RetrievalQA.from_chain_type(\n",
        "    llm=llm, chain_type=\"stuff\", retriever=retriever, verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eEosQNUUBaEI"
      },
      "source": [
        "Test RAG\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "29bx7cTTBcBE"
      },
      "outputs": [],
      "source": [
        "def test_rag(qa, query):\n",
        "    print(f\"Query: {query}\\n\")\n",
        "    time_1 = time()\n",
        "    result = qa.run(query)\n",
        "    time_2 = time()\n",
        "    print(f\"Inference time: {round(time_2-time_1, 3)} sec.\")\n",
        "    print(\"\\nResult: \", result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61QmD99hBtsh"
      },
      "source": [
        "Get sources...\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y5smrotNBwBG",
        "outputId": "90d96bab-f549-4aae-b24a-c844fd90c0de"
      },
      "outputs": [],
      "source": [
        "query = \"Which scientific article should I read to learn about RFdiffusion for protein folding?\"\n",
        "docs = vectordb.similarity_search(query)\n",
        "print(f\"Query: {query}\")\n",
        "print(f\"Retrieved documents: {len(docs)}\")\n",
        "for doc in docs:\n",
        "    doc_details = doc.to_json()[\"kwargs\"]\n",
        "    print(\"Source: \", doc_details[\"metadata\"][\"source\"])\n",
        "    print(\"Text: \", doc_details[\"page_content\"], \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oIPlwM3-BgYT",
        "outputId": "7656e3bf-8d3c-4939-9cf9-cecd5f92ad3d"
      },
      "outputs": [],
      "source": [
        "query = \"What were the main topics in the State of the Union in 2023? Summarize. Keep it under 200 words.\"\n",
        "test_rag(qa, query)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0b90e32e0f4449c78f63832f77aa0184": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "220dd6c6142f49309b580befad8037c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_770c05476b3843a0ac73e8bca990453a",
            "placeholder": "​",
            "style": "IPY_MODEL_b06c2e4efe934233ba086b7d2863334e",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "770c05476b3843a0ac73e8bca990453a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8445cffea19548d5bc7d8892f20915da": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ac3f3d53dd7f45c18a8b792c2d8e4239",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dec521676dff47b1b594d1fd891ec81c",
            "value": 2
          }
        },
        "a7dc5828777e4a40adb816e9a3d0c11c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac3f3d53dd7f45c18a8b792c2d8e4239": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b06c2e4efe934233ba086b7d2863334e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d59d898fff02424baef83b8fcfa6b70f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dec521676dff47b1b594d1fd891ec81c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e9286683b7e34ad5a330f02bccf406f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d59d898fff02424baef83b8fcfa6b70f",
            "placeholder": "​",
            "style": "IPY_MODEL_0b90e32e0f4449c78f63832f77aa0184",
            "value": " 2/2 [00:58&lt;00:00, 26.63s/it]"
          }
        },
        "f212bf7772464da3a3f2e9078ed4e2e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_220dd6c6142f49309b580befad8037c8",
              "IPY_MODEL_8445cffea19548d5bc7d8892f20915da",
              "IPY_MODEL_e9286683b7e34ad5a330f02bccf406f4"
            ],
            "layout": "IPY_MODEL_a7dc5828777e4a40adb816e9a3d0c11c"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
