{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/brettin/llm_tutorial/blob/main/tutorials/05-chains/05_langchain_rag.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3fahVg9F5GMN",
        "outputId": "455e7917-89c5-4fc8-8095-fcc778b3aa90"
      },
      "outputs": [],
      "source": [
        "!pip install -U langchain langchain-community openai chromadb langchainhub bs4 tiktoken kaleido python-multipart cohere"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "renzXk8Z5npr"
      },
      "outputs": [],
      "source": [
        "import bs4\n",
        "from langchain import hub\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.document_loaders import WebBaseLoader\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.schema import StrOutputParser\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x3fjPVxE581L",
        "outputId": "3f0efe1c-4815-4530-cb5b-87ecd9f48af3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "os.environ['HUGGINGFACEHUB_API_TOKEN'] = getpass('Enter HUGGINGFACEHUB_API_TOKEN: ')\n",
        "os.environ['OPENAI_API_KEY'] = getpass(\"Enter OPENAI_API_KEY: \")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This exercise demonstrates how to create a basic Retrieval-Augmented Generation (RAG) pipeline using various components in Python. The goal is to build a system that retrieves relevant information from a webpage and uses it to answer a specific question with a language model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 1: Load the Webpage Content\n",
        "1.\tWebBaseLoader: This class is used to load content from a web page. The web_paths argument specifies the URL of the webpage you want to load. In this case, it’s a blog post by Lilian Weng on “agent.”\n",
        "2.\tBeautifulSoup (bs_kwargs): The bs_kwargs argument is used to filter the content being scraped from the webpage. The parse_only parameter specifies that only certain HTML elements with classes \"post-content\", \"post-title\", and \"post-header\" should be parsed. This helps in focusing on the relevant content, ignoring other unnecessary parts of the webpage like advertisements or navigation menus.\n",
        "3.\tLoading Documents: The loader.load() method fetches and parses the content from the specified webpage, storing it in the docs variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "loader = WebBaseLoader(\n",
        "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
        "    bs_kwargs=dict(\n",
        "        parse_only=bs4.SoupStrainer(\n",
        "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
        "        )\n",
        "    ),\n",
        ")\n",
        "docs = loader.load()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2: Split the Documents\n",
        "1.\tRecursiveCharacterTextSplitter: This is a utility that splits the loaded documents into smaller chunks. This is useful because language models typically have a token limit, and large documents may exceed this limit.\n",
        "2.\tchunk_size: Specifies the maximum size of each chunk, in characters. Here, it’s set to 1000 characters.\n",
        "3.\tchunk_overlap: Specifies the number of overlapping characters between consecutive chunks, ensuring some continuity between them. Here, it’s set to 200 characters.\n",
        "4.\tSplitting the Documents: The split_documents(docs) method applies this logic to the loaded document, resulting in a list of smaller text chunks stored in the splits variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "es0FpcNt5t_x",
        "outputId": "3dcc723d-810b-401b-bea4-68e5e2f26e8c"
      },
      "outputs": [],
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "splits = text_splitter.split_documents(docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 3: Create a Vector Store and Embed the Text Chunks\n",
        "1.\tChroma: Chroma is a vector store that allows for efficient storage and retrieval of vectorized documents. A vector store is a database optimized for storing and querying high-dimensional vectors.\n",
        "2.\tOpenAIEmbeddings: This is an embedding model from OpenAI used to convert chunks of text into high-dimensional vectors. These vectors capture the semantic meaning of the text, enabling similarity searches.\n",
        "3.\tfrom_documents: The from_documents method converts the text chunks (splits) into vectors using the specified embedding model and stores them in the vectorstore.\n",
        "4.\tas_retriever: This method converts the vector store into a retriever object, which can be used to fetch relevant documents based on a query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
        "retriever = vectorstore.as_retriever()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 4: Prepare RAG Pipeline\n",
        "1.\tPrompt Template: The hub.pull(\"rlm/rag-prompt\") retrieves a pre-built prompt template for RAG (Retrieval-Augmented Generation). This template will guide how the question and the retrieved documents are fed into the language model. The print(prompt) statement is commented out, but if uncommented, it would display the prompt template.\n",
        "2.\tChatOpenAI: This is the language model (in this case, GPT-4o Mini from OpenAI) that will be used to generate responses. The temperature parameter is set to 0, making the model deterministic and less creative, which is useful for factual and precise responses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt = hub.pull(\"rlm/rag-prompt\")\n",
        "# print(prompt)\n",
        "llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 5: Define the RAG Chain\n",
        "1.\tformat_docs Function: This function takes a list of documents (or chunks) and formats them into a single string by joining their content with newlines in between. This formatted string will be used as the context for answering the question.\n",
        "2.\tRAG Chain: This is the heart of the RAG pipeline. It is a sequence of operations that:\n",
        "- Uses the retriever to fetch relevant chunks of text based on the query.\n",
        "- Formats these retrieved chunks using the format_docs function.\n",
        "- Passes the formatted text along with the original question to the prompt.\n",
        "- Feeds the output of the prompt into the language model (llm).\n",
        "- Parses the model’s output using StrOutputParser() to produce a clean and readable answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "\n",
        "rag_chain = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 6: Invoke the RAG Pipeline\n",
        "1.\tinvoke Method: The invoke method triggers the entire RAG pipeline. Here, it’s called with the question \"What is Task Decomposition?\".\n",
        "2.\tOutput: The system retrieves relevant information from the loaded webpage, processes it, and generates a coherent answer using the language model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rag_chain.invoke(\"What is Task Decomposition?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this exercise, we will implement a RAG pipeline using a custom loader to fetch academic articles from the arXiv repository and a language model to generate answers to questions based on the retrieved content."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wafZi4IDxkXe",
        "outputId": "d6f8a86d-0b37-4f8d-b9c0-bb30112cf204"
      },
      "outputs": [],
      "source": [
        "!pip install arxiv\n",
        "!pip install pymupdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 1: Load Academic Papers from arXiv\n",
        "- ArxivLoader: This component is used to load academic papers from the arXiv repository.\n",
        "- query: The search query used to find relevant papers is \"Antibiotic design using deep learning\".\n",
        "- load_max_docs=20: Limits the maximum number of documents to be loaded to 20.\n",
        "- .load(): Executes the loading of the documents, which are returned as a list of Document objects."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YlXvJFhZioqn"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import ArxivLoader\n",
        "from langchain.retrievers import ArxivRetriever\n",
        "\n",
        "# cleanup previous\n",
        "# vectorstore.delete_collection()\n",
        "\n",
        "docs = ArxivLoader(query=\"Antibiotic design using deep learning\", load_max_docs=20).load()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2: Split the Documents\n",
        "- RecursiveCharacterTextSplitter: This component splits documents into smaller chunks, which is useful for efficient processing in subsequent steps.\n",
        "- chunk_size=1000: Each chunk will contain up to 1000 characters.\n",
        "- chunk_overlap=200: Overlaps chunks by 200 characters to maintain context across chunks.\n",
        "- .split_documents(docs): Splits the loaded documents (docs) into smaller chunks, stored in splits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "splits = text_splitter.split_documents(docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 3: Create a Vector Store and Embed the Text Chunks\n",
        "- Chroma: A vector store implementation used to store and retrieve documents efficiently.\n",
        "- from_documents: This method creates a vector store from the split document chunks (splits).\n",
        "- OpenAIEmbeddings(): Generates embeddings (vector representations) of the document chunks using OpenAI’s embedding models.\n",
        "- as_retriever(): Converts the vector store into a retriever that can be queried to find relevant documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
        "retriever = vectorstore.as_retriever()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 4: Prepare RAG Pipeline\n",
        "- prompt: A pre-configured prompt is pulled from a hub (possibly Hugging Face or similar) for guiding the response generation.\n",
        "- ChatOpenAI: Initializes the language model (in this case, GPT-4o Mini) for generating text responses.\n",
        "- model_name=“gpt-4o-mini: Specifies the exact version of the GPT model.\n",
        "- temperature=0: Indicates deterministic output (no randomness) in responses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt = hub.pull(\"rlm/rag-prompt\")\n",
        "llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 5: Document Formatting Function\n",
        "- format_docs(docs): This function formats the retrieved documents into a single string, separating each document’s content with two newlines."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 6: Define the RAG Chain\n",
        "- rag_chain: This is a pipeline that performs the following:\n",
        "    - retriever | format_docs: Retrieves relevant documents and formats them.\n",
        "    - question: Takes an input question, passed directly using RunnablePassthrough().\n",
        "    - prompt: Passes the context and question to the pre-configured prompt.\n",
        "    - llm: The prompt is then passed to the language model, which generates a response.\n",
        "    - StrOutputParser(): Converts the model’s output into a string format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rag_chain = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 7: Invoke the RAG Pipeline\n",
        "- rag_chain.invoke(…): Executes the RAG pipeline with the specified question, which in this case is about summarizing the current state of using deep learning in antibiotic discovery."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "resp = rag_chain.invoke(\n",
        "  \"\"\"\n",
        "  Can you provide a summary of the current state of applying\n",
        "  deep learning to the discovery of new antibiotics?\n",
        "  \"\"\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 135
        },
        "id": "2WokPGM7dzdF",
        "outputId": "08aaaa0b-1bc2-4db1-9b1a-6c62c4990420"
      },
      "outputs": [],
      "source": [
        "# Return the response in readable format\n",
        "resp.format()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s8bsLyMFxDk8"
      },
      "outputs": [],
      "source": [
        "# Cleanup\n",
        "vectorstore.delete_collection()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyP+1qfMjP6dZ3MFqWpCEc7g",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
